"""
Open Canvas 共享工具函数

包含模型配置、格式化、上下文处理等核心工具
"""

import json
import os
import uuid
from typing import Any, Optional

from langchain_core.messages import AIMessage, BaseMessage
from langgraph.store.base import BaseStore
from langgraph.types import RunnableConfig

from .constants import (
    CONTEXT_DOCUMENTS_NAMESPACE,
    DEFAULT_INPUTS,
    OC_HIDE_FROM_UI_KEY,
    OC_SUMMARIZED_MESSAGE_KEY,
    OC_WEB_SEARCH_RESULTS_MESSAGE_KEY,
    PROGRAMMING_LANGUAGES,
    TEMPERATURE_EXCLUDED_MODELS,
)
from .types import (
    ArtifactCodeV3,
    ArtifactMarkdownV3,
    CustomModelConfig,
    Reflections,
    SearchResult,
)


# ============================================
# 辅助函数
# ============================================


def is_artifact_code_content(
    content: ArtifactMarkdownV3 | ArtifactCodeV3,
) -> bool:
    """判断工件内容是否为代码类型"""
    return content.get("type") == "code"


# ============================================
# 反思格式化
# ============================================


def format_reflections(
    reflections: Reflections,
    only_style: bool = False,
    only_content: bool = False,
) -> str:
    """
    格式化用户反思/记忆为提示词字符串

    Args:
        reflections: 用户反思数据
        only_style: 仅返回风格规则
        only_content: 仅返回用户事实

    Returns:
        格式化的反思字符串
    """
    if only_style and only_content:
        raise ValueError("Cannot specify both `only_style` and `only_content` as True.")

    # 处理 styleRules
    style_rules = reflections.get("styleRules", [])
    if isinstance(style_rules, str):
        try:
            style_rules = json.loads(style_rules)
        except json.JSONDecodeError:
            style_rules = []

    style_rules_str = (
        "\n- ".join(style_rules) if style_rules else "No style guidelines found."
    )

    # 处理 content
    content_rules = reflections.get("content", [])
    if isinstance(content_rules, str):
        try:
            content_rules = json.loads(content_rules)
        except json.JSONDecodeError:
            content_rules = []

    content_rules_str = (
        "\n- ".join(content_rules) if content_rules else "No memories/facts found."
    )

    style_string = f"""The following is a list of style guidelines previously generated by you:
<style-guidelines>
- {style_rules_str}
</style-guidelines>"""

    content_string = f"""The following is a list of memories/facts you previously generated about the user:
<user-facts>
- {content_rules_str}
</user-facts>"""

    if only_style:
        return style_string
    if only_content:
        return content_string

    return style_string + "\n\n" + content_string


def ensure_store_in_config(config: RunnableConfig) -> BaseStore:
    """确保配置中包含 store"""
    store = config.get("store")
    if not store:
        raise ValueError("`store` not found in config")
    return store


async def get_formatted_reflections(config: RunnableConfig) -> str:
    """
    从 store 获取并格式化用户反思

    Args:
        config: LangGraph 运行配置

    Returns:
        格式化的反思字符串
    """
    store = config.get("store")
    if not store:
        return "No reflections found."

    assistant_id = config.get("configurable", {}).get("assistant_id")
    if not assistant_id:
        raise ValueError("`assistant_id` not found in configurable")

    memory_namespace = ["memories", assistant_id]
    memory_key = "reflection"

    memories = await store.aget(memory_namespace, memory_key)
    if memories and memories.value:
        return format_reflections(memories.value)

    return "No reflections found."


# ============================================
# 工件格式化
# ============================================


def format_artifact_content(
    content: ArtifactMarkdownV3 | ArtifactCodeV3,
    shorten_content: bool = False,
) -> str:
    """
    格式化工件内容为字符串

    Args:
        content: 工件内容
        shorten_content: 是否截断内容（前500字符）

    Returns:
        格式化的工件字符串
    """
    if is_artifact_code_content(content):
        artifact_content = content.get("code", "")
        if shorten_content:
            artifact_content = artifact_content[:500]
    else:
        artifact_content = content.get("fullMarkdown", "")
        if shorten_content:
            artifact_content = artifact_content[:500]

    return f"Title: {content.get('title', 'Untitled')}\nArtifact type: {content.get('type', 'unknown')}\nContent: {artifact_content}"


def format_artifact_content_with_template(
    template: str,
    content: ArtifactMarkdownV3 | ArtifactCodeV3,
    shorten_content: bool = False,
) -> str:
    """
    使用模板格式化工件内容

    Args:
        template: 包含 {artifact} 占位符的模板
        content: 工件内容
        shorten_content: 是否截断内容

    Returns:
        替换后的模板字符串
    """
    return template.replace("{artifact}", format_artifact_content(content, shorten_content))


# ============================================
# 模型配置
# ============================================


def get_model_config(
    config: RunnableConfig,
    is_tool_calling: bool = False,
) -> dict[str, Any]:
    """
    从配置中获取模型配置

    Args:
        config: LangGraph 运行配置
        is_tool_calling: 是否用于工具调用（某些模型不支持）

    Returns:
        包含 modelName, modelProvider, apiKey 等的字典
    """
    configurable = config.get("configurable", {})
    custom_model_name: str = configurable.get("customModelName", "")

    if not custom_model_name:
        raise ValueError("Model name is missing in config.")

    model_config: Optional[CustomModelConfig] = configurable.get("modelConfig")

    # Azure OpenAI
    if custom_model_name.startswith("azure/"):
        actual_model_name = custom_model_name.replace("azure/", "")
        if is_tool_calling and "o1" in actual_model_name:
            actual_model_name = "gpt-4o"

        return {
            "modelName": actual_model_name,
            "modelProvider": "azure_openai",
            "modelConfig": model_config,
            "azureConfig": {
                "azureOpenAIApiKey": os.environ.get("_AZURE_OPENAI_API_KEY", ""),
                "azureOpenAIApiInstanceName": os.environ.get(
                    "_AZURE_OPENAI_API_INSTANCE_NAME", ""
                ),
                "azureOpenAIApiDeploymentName": os.environ.get(
                    "_AZURE_OPENAI_API_DEPLOYMENT_NAME", ""
                ),
                "azureOpenAIApiVersion": os.environ.get(
                    "_AZURE_OPENAI_API_VERSION", "2024-08-01-preview"
                ),
                "azureOpenAIBasePath": os.environ.get("_AZURE_OPENAI_API_BASE_PATH"),
            },
        }

    base_config = {
        "modelName": custom_model_name,
        "modelConfig": model_config,
    }

    # OpenAI (gpt-*, o1, o3, o4)
    if any(
        x in custom_model_name
        for x in ["gpt-", "o1", "o3", "o4"]
    ):
        actual_model_name = custom_model_name
        if is_tool_calling and "o1" in actual_model_name:
            actual_model_name = "gpt-4o"

        return {
            **base_config,
            "modelName": actual_model_name,
            "modelProvider": "openai",
            "apiKey": os.environ.get("OPENAI_API_KEY"),
        }

    # Anthropic (claude-*)
    if "claude-" in custom_model_name:
        return {
            **base_config,
            "modelProvider": "anthropic",
            "apiKey": os.environ.get("ANTHROPIC_API_KEY"),
        }

    # Fireworks (accounts/fireworks/models/xxx 格式)
    if "fireworks/" in custom_model_name:
        actual_model_name = custom_model_name
        if is_tool_calling and actual_model_name != "accounts/fireworks/models/llama-v3p3-70b-instruct":
            actual_model_name = "accounts/fireworks/models/llama-v3p3-70b-instruct"

        return {
            **base_config,
            "modelName": actual_model_name,
            "modelProvider": "fireworks",
            "apiKey": os.environ.get("FIREWORKS_API_KEY"),
        }

    # Groq
    if custom_model_name.startswith("groq/"):
        actual_model_name = custom_model_name.replace("groq/", "")
        return {
            "modelName": actual_model_name,
            "modelProvider": "groq",
            "apiKey": os.environ.get("GROQ_API_KEY"),
        }

    # Google Gemini
    if "gemini-" in custom_model_name:
        actual_model_name = custom_model_name
        if is_tool_calling and "thinking" in actual_model_name:
            actual_model_name = "gemini-2.0-flash-exp"

        return {
            **base_config,
            "modelName": actual_model_name,
            "modelProvider": "google-genai",
            "apiKey": os.environ.get("GOOGLE_API_KEY"),
        }

    # Ollama
    if custom_model_name.startswith("ollama-"):
        return {
            "modelName": custom_model_name.replace("ollama-", ""),
            "modelProvider": "ollama",
            "baseUrl": os.environ.get("OLLAMA_API_URL", "http://host.docker.internal:11434"),
        }

    raise ValueError(f"Unknown model provider for model: {custom_model_name}")


def get_model_from_config(
    config: RunnableConfig,
    is_tool_calling: bool = False,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
):
    """
    根据配置创建并返回 LLM 模型实例

    Args:
        config: LangGraph 运行配置
        is_tool_calling: 是否用于工具调用
        temperature: 可选温度参数 (覆盖 modelConfig 默认值)
        max_tokens: 可选最大 token 数 (覆盖 modelConfig 默认值)

    Returns:
        BaseChatModel 实例

    Note:
        - 从 modelConfig 读取默认值，extra 参数可覆盖
        - 推理模型 (o1, o3 系列) 使用 max_completion_tokens 而非 max_tokens
        - Anthropic 需要覆盖默认的 top_p/top_k 避免 API 错误
    """
    model_cfg = get_model_config(config, is_tool_calling)
    provider = model_cfg.get("modelProvider", "")
    model_name = model_cfg.get("modelName", "")
    model_config = model_cfg.get("modelConfig")

    # ============================================
    # 从 modelConfig 读取默认值，extra 参数覆盖
    # ============================================
    final_temperature = temperature
    if final_temperature is None and model_config:
        temp_range = model_config.get("temperatureRange", {})
        if temp_range:
            final_temperature = temp_range.get("current", 0.5)

    final_max_tokens = max_tokens
    if final_max_tokens is None and model_config:
        tokens_range = model_config.get("maxTokens", {})
        if tokens_range:
            final_max_tokens = tokens_range.get("current")

    # ============================================
    # 判断是否为推理模型 (使用 max_completion_tokens)
    # ============================================
    is_reasoning_model = model_name in TEMPERATURE_EXCLUDED_MODELS

    # 构建通用参数
    if is_reasoning_model:
        # 推理模型: 不支持 temperature，使用 max_completion_tokens
        kwargs: dict[str, Any] = {"model": model_name}
        if final_max_tokens is not None:
            kwargs["max_completion_tokens"] = final_max_tokens
    else:
        # 普通模型: 支持 temperature 和 max_tokens
        kwargs = {"model": model_name}
        if final_temperature is not None:
            kwargs["temperature"] = final_temperature
        if final_max_tokens is not None:
            kwargs["max_tokens"] = final_max_tokens

    # ============================================
    # 根据 provider 创建模型
    # ============================================
    if provider == "openai":
        from langchain_openai import ChatOpenAI

        return ChatOpenAI(**kwargs)

    elif provider == "azure_openai":
        from langchain_openai import AzureChatOpenAI

        azure_cfg = model_cfg.get("azureConfig", {})
        return AzureChatOpenAI(
            model=model_name,
            azure_endpoint=f"https://{azure_cfg.get('azureOpenAIApiInstanceName')}.openai.azure.com",
            api_key=azure_cfg.get("azureOpenAIApiKey"),
            api_version=azure_cfg.get("azureOpenAIApiVersion"),
            azure_deployment=azure_cfg.get("azureOpenAIApiDeploymentName"),
            temperature=kwargs.get("temperature"),
            max_tokens=kwargs.get("max_tokens"),
        )

    elif provider == "anthropic":
        from langchain_anthropic import ChatAnthropic

        # 重要: 覆盖 ChatAnthropic 默认的 top_p/top_k 避免 API 错误
        # ChatAnthropic 默认设置 top_p=1, top_k=1，这会与某些 API 配置冲突
        # 通过 model_kwargs 显式设置为 None 来覆盖
        anthropic_kwargs = {
            "model": model_name,
            "model_kwargs": {
                "top_p": None,
                "top_k": None,
            },
        }
        if not is_reasoning_model and final_temperature is not None:
            anthropic_kwargs["temperature"] = final_temperature
        if final_max_tokens is not None:
            anthropic_kwargs["max_tokens"] = final_max_tokens

        return ChatAnthropic(**anthropic_kwargs)

    elif provider == "google-genai":
        from langchain_google_genai import ChatGoogleGenerativeAI

        return ChatGoogleGenerativeAI(**kwargs)

    elif provider == "fireworks":
        from langchain_fireworks import ChatFireworks

        return ChatFireworks(**kwargs)

    elif provider == "groq":
        # Groq 使用 langchain-groq 包
        try:
            from langchain_groq import ChatGroq

            return ChatGroq(**kwargs)
        except ImportError:
            raise ImportError("langchain-groq package is required for Groq models")

    elif provider == "ollama":
        from langchain_ollama import ChatOllama

        base_url = model_cfg.get("baseUrl", "http://localhost:11434")
        return ChatOllama(model=model_name, base_url=base_url)

    else:
        raise ValueError(f"Unknown model provider: {provider}")


def optionally_get_system_prompt_from_config(config: RunnableConfig) -> Optional[str]:
    """从配置中获取自定义系统提示（如果有）"""
    return config.get("configurable", {}).get("systemPrompt")


def is_using_o1_mini_model(config: RunnableConfig) -> bool:
    """检查是否使用 o1-mini 模型"""
    model_config = get_model_config(config)
    return "o1-mini" in model_config.get("modelName", "")


# ============================================
# 消息格式化
# ============================================


def format_messages(messages: list[BaseMessage]) -> str:
    """
    格式化消息列表为 XML 字符串

    Args:
        messages: 消息列表

    Returns:
        XML 格式的消息字符串
    """
    formatted = []
    for idx, msg in enumerate(messages):
        msg_type = msg.type if hasattr(msg, "type") else "unknown"
        content = msg.content if isinstance(msg.content, str) else ""

        if not isinstance(msg.content, str) and hasattr(msg.content, "__iter__"):
            content = "\n".join(
                c.get("text", "") for c in msg.content if isinstance(c, dict) and "text" in c
            )

        formatted.append(f'<{msg_type} index="{idx}">\n{content}\n</{msg_type}>')

    return "\n".join(formatted)


def get_string_from_content(content: str | list[dict[str, Any]]) -> str:
    """从消息内容中提取字符串"""
    if isinstance(content, str):
        return content

    return "\n".join(
        c.get("text", "") for c in content if isinstance(c, dict) and "text" in c
    )


# ============================================
# 网络搜索结果处理
# ============================================


def create_ai_message_from_web_results(web_results: list[SearchResult]) -> AIMessage:
    """
    从网络搜索结果创建 AI 消息

    Args:
        web_results: 搜索结果列表 (嵌套结构: pageContent + metadata)

    Returns:
        包含搜索结果的 AIMessage

    Note:
        SearchResult 使用嵌套结构匹配 TS DocumentInterface<ExaMetadata>:
        - pageContent: 顶层字段
        - metadata: 包含 id, url, title, author, publishedDate 等
    """
    web_results_str = "\n\n".join(
        f"""<search-result
      index="{index}"
      publishedDate="{r.get('metadata', {}).get('publishedDate', 'Unknown')}"
      author="{r.get('metadata', {}).get('author', 'Unknown')}"
    >
      [{r.get('metadata', {}).get('title', 'Unknown title')}]({r.get('metadata', {}).get('url', 'Unknown URL')})
      {r.get('pageContent', '')}
    </search-result>"""
        for index, r in enumerate(web_results)
    )

    content = f"Here is some additional context I found from searching the web. This may be useful:\n\n{web_results_str}"

    return AIMessage(
        content=content,
        id=f"web-search-results-{uuid.uuid4()}",
        additional_kwargs={
            OC_WEB_SEARCH_RESULTS_MESSAGE_KEY: True,
            "webSearchResults": web_results,
            "webSearchStatus": "done",
        },
    )


# 注意: PROGRAMMING_LANGUAGES 已移至 constants.py
# 为保持向后兼容，通过 from .constants import PROGRAMMING_LANGUAGES 导入并重新导出
