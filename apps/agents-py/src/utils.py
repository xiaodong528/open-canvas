"""
Open Canvas 共享工具函数

包含模型配置、格式化、上下文处理等核心工具
"""

import json
import os
import uuid
from typing import Any, Optional

from langchain_core.messages import AIMessage, BaseMessage
from langgraph.store.base import BaseStore
from langgraph.types import RunnableConfig

from .constants import (
    CONTEXT_DOCUMENTS_NAMESPACE,
    DEFAULT_INPUTS,
    OC_HIDE_FROM_UI_KEY,
    OC_SUMMARIZED_MESSAGE_KEY,
    OC_WEB_SEARCH_RESULTS_MESSAGE_KEY,
    PROGRAMMING_LANGUAGES,
    TEMPERATURE_EXCLUDED_MODELS,
)
from .types import (
    ArtifactCodeV3,
    ArtifactMarkdownV3,
    CustomModelConfig,
    Reflections,
    SearchResult,
)


# ============================================
# 辅助函数
# ============================================


def is_artifact_code_content(
    content: ArtifactMarkdownV3 | ArtifactCodeV3,
) -> bool:
    """判断工件内容是否为代码类型"""
    return content.get("type") == "code"


# ============================================
# 反思格式化
# ============================================


def format_reflections(
    reflections: Reflections,
    only_style: bool = False,
    only_content: bool = False,
) -> str:
    """
    格式化用户反思/记忆为提示词字符串

    Args:
        reflections: 用户反思数据
        only_style: 仅返回风格规则
        only_content: 仅返回用户事实

    Returns:
        格式化的反思字符串
    """
    if only_style and only_content:
        raise ValueError("Cannot specify both `only_style` and `only_content` as True.")

    # 处理 styleRules
    style_rules = reflections.get("styleRules", [])
    if isinstance(style_rules, str):
        try:
            style_rules = json.loads(style_rules)
        except json.JSONDecodeError:
            style_rules = []

    style_rules_str = (
        "\n- ".join(style_rules) if style_rules else "No style guidelines found."
    )

    # 处理 content
    content_rules = reflections.get("content", [])
    if isinstance(content_rules, str):
        try:
            content_rules = json.loads(content_rules)
        except json.JSONDecodeError:
            content_rules = []

    content_rules_str = (
        "\n- ".join(content_rules) if content_rules else "No memories/facts found."
    )

    style_string = f"""The following is a list of style guidelines previously generated by you:
<style-guidelines>
- {style_rules_str}
</style-guidelines>"""

    content_string = f"""The following is a list of memories/facts you previously generated about the user:
<user-facts>
- {content_rules_str}
</user-facts>"""

    if only_style:
        return style_string
    if only_content:
        return content_string

    return style_string + "\n\n" + content_string


def ensure_store_in_config(config: RunnableConfig) -> BaseStore:
    """确保配置中包含 store"""
    store = config.get("store")
    if not store:
        raise ValueError("`store` not found in config")
    return store


async def get_formatted_reflections(config: RunnableConfig) -> str:
    """
    从 store 获取并格式化用户反思

    Args:
        config: LangGraph 运行配置

    Returns:
        格式化的反思字符串
    """
    store = config.get("store")
    if not store:
        return "No reflections found."

    assistant_id = config.get("configurable", {}).get("assistant_id")
    if not assistant_id:
        raise ValueError("`assistant_id` not found in configurable")

    memory_namespace = ("memories", assistant_id)
    memory_key = "reflection"

    memories = await store.aget(memory_namespace, memory_key)
    if memories and memories.value:
        return format_reflections(memories.value)

    return "No reflections found."


# ============================================
# 工件格式化
# ============================================


def format_artifact_content(
    content: ArtifactMarkdownV3 | ArtifactCodeV3,
    shorten_content: bool = False,
) -> str:
    """
    格式化工件内容为字符串

    Args:
        content: 工件内容
        shorten_content: 是否截断内容（前500字符）

    Returns:
        格式化的工件字符串
    """
    if is_artifact_code_content(content):
        artifact_content = content.get("code", "")
        if shorten_content:
            artifact_content = artifact_content[:500]
    else:
        artifact_content = content.get("fullMarkdown", "")
        if shorten_content:
            artifact_content = artifact_content[:500]

    return f"Title: {content.get('title', 'Untitled')}\nArtifact type: {content.get('type', 'unknown')}\nContent: {artifact_content}"


def format_artifact_content_with_template(
    template: str,
    content: ArtifactMarkdownV3 | ArtifactCodeV3,
    shorten_content: bool = False,
) -> str:
    """
    使用模板格式化工件内容

    Args:
        template: 包含 {artifact} 占位符的模板
        content: 工件内容
        shorten_content: 是否截断内容

    Returns:
        替换后的模板字符串
    """
    return template.replace("{artifact}", format_artifact_content(content, shorten_content))


# ============================================
# 模型配置
# ============================================


def get_model_config(
    config: RunnableConfig,
    is_tool_calling: bool = False,
) -> dict[str, Any]:
    """
    从配置中获取模型配置

    Args:
        config: LangGraph 运行配置
        is_tool_calling: 是否用于工具调用（某些模型不支持）

    Returns:
        包含 modelName, modelProvider, apiKey 等的字典
    """
    configurable = config.get("configurable", {})
    custom_model_name: str = configurable.get("customModelName", "")

    if not custom_model_name:
        raise ValueError("Model name is missing in config.")

    model_config: Optional[CustomModelConfig] = configurable.get("modelConfig")

    # Azure OpenAI
    if custom_model_name.startswith("azure/"):
        actual_model_name = custom_model_name.replace("azure/", "")
        if is_tool_calling and "o1" in actual_model_name:
            actual_model_name = "gpt-4o"

        return {
            "modelName": actual_model_name,
            "modelProvider": "azure_openai",
            "modelConfig": model_config,
            "azureConfig": {
                "azureOpenAIApiKey": os.environ.get("_AZURE_OPENAI_API_KEY", ""),
                "azureOpenAIApiInstanceName": os.environ.get(
                    "_AZURE_OPENAI_API_INSTANCE_NAME", ""
                ),
                "azureOpenAIApiDeploymentName": os.environ.get(
                    "_AZURE_OPENAI_API_DEPLOYMENT_NAME", ""
                ),
                "azureOpenAIApiVersion": os.environ.get(
                    "_AZURE_OPENAI_API_VERSION", "2024-08-01-preview"
                ),
                "azureOpenAIBasePath": os.environ.get("_AZURE_OPENAI_API_BASE_PATH"),
            },
        }

    base_config = {
        "modelName": custom_model_name,
        "modelConfig": model_config,
    }

    # OpenAI (gpt-*, o1, o3, o4)
    if any(
        x in custom_model_name
        for x in ["gpt-", "o1", "o3", "o4"]
    ):
        actual_model_name = custom_model_name
        if is_tool_calling and "o1" in actual_model_name:
            actual_model_name = "gpt-4o"

        return {
            **base_config,
            "modelName": actual_model_name,
            "modelProvider": "openai",
            "apiKey": os.environ.get("OPENAI_API_KEY"),
        }

    # Anthropic (claude-*)
    if "claude-" in custom_model_name:
        return {
            **base_config,
            "modelProvider": "anthropic",
            "apiKey": os.environ.get("ANTHROPIC_API_KEY"),
        }

    # Fireworks (accounts/fireworks/models/xxx 格式)
    if "fireworks/" in custom_model_name:
        actual_model_name = custom_model_name
        if is_tool_calling and actual_model_name != "accounts/fireworks/models/llama-v3p3-70b-instruct":
            actual_model_name = "accounts/fireworks/models/llama-v3p3-70b-instruct"

        return {
            **base_config,
            "modelName": actual_model_name,
            "modelProvider": "fireworks",
            "apiKey": os.environ.get("FIREWORKS_API_KEY"),
        }

    # Groq
    if custom_model_name.startswith("groq/"):
        actual_model_name = custom_model_name.replace("groq/", "")
        return {
            "modelName": actual_model_name,
            "modelProvider": "groq",
            "apiKey": os.environ.get("GROQ_API_KEY"),
        }

    # Google Gemini
    if "gemini-" in custom_model_name:
        actual_model_name = custom_model_name
        if is_tool_calling and "thinking" in actual_model_name:
            actual_model_name = "gemini-2.0-flash-exp"

        return {
            **base_config,
            "modelName": actual_model_name,
            "modelProvider": "google-genai",
            "apiKey": os.environ.get("GOOGLE_API_KEY"),
        }

    # Ollama
    if custom_model_name.startswith("ollama-"):
        return {
            "modelName": custom_model_name.replace("ollama-", ""),
            "modelProvider": "ollama",
            "baseUrl": os.environ.get("OLLAMA_API_URL", "http://host.docker.internal:11434"),
        }

    raise ValueError(f"Unknown model provider for model: {custom_model_name}")


def get_model_from_config(
    config: RunnableConfig,
    is_tool_calling: bool = False,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
):
    """
    根据配置创建并返回 LLM 模型实例

    Args:
        config: LangGraph 运行配置
        is_tool_calling: 是否用于工具调用
        temperature: 可选温度参数 (覆盖 modelConfig 默认值)
        max_tokens: 可选最大 token 数 (覆盖 modelConfig 默认值)

    Returns:
        BaseChatModel 实例

    Note:
        - 从 modelConfig 读取默认值，extra 参数可覆盖
        - 推理模型 (o1, o3 系列) 使用 max_completion_tokens 而非 max_tokens
        - Anthropic 需要覆盖默认的 top_p/top_k 避免 API 错误
    """
    model_cfg = get_model_config(config, is_tool_calling)
    provider = model_cfg.get("modelProvider", "")
    model_name = model_cfg.get("modelName", "")
    model_config = model_cfg.get("modelConfig")

    # ============================================
    # 从 modelConfig 读取默认值，extra 参数覆盖
    # ============================================
    final_temperature = temperature
    if final_temperature is None and model_config:
        temp_range = model_config.get("temperatureRange", {})
        if temp_range:
            final_temperature = temp_range.get("current", 0.5)

    final_max_tokens = max_tokens
    if final_max_tokens is None and model_config:
        tokens_range = model_config.get("maxTokens", {})
        if tokens_range:
            final_max_tokens = tokens_range.get("current")

    # ============================================
    # 判断是否为推理模型 (使用 max_completion_tokens)
    # ============================================
    is_reasoning_model = model_name in TEMPERATURE_EXCLUDED_MODELS

    # 构建通用参数
    if is_reasoning_model:
        # 推理模型: 不支持 temperature，使用 max_completion_tokens
        kwargs: dict[str, Any] = {"model": model_name}
        if final_max_tokens is not None:
            kwargs["max_completion_tokens"] = final_max_tokens
    else:
        # 普通模型: 支持 temperature 和 max_tokens
        kwargs = {"model": model_name}
        if final_temperature is not None:
            kwargs["temperature"] = final_temperature
        if final_max_tokens is not None:
            kwargs["max_tokens"] = final_max_tokens

    # ============================================
    # 根据 provider 创建模型
    # ============================================
    if provider == "openai":
        from langchain_openai import ChatOpenAI

        return ChatOpenAI(**kwargs)

    elif provider == "azure_openai":
        from langchain_openai import AzureChatOpenAI

        azure_cfg = model_cfg.get("azureConfig", {})
        return AzureChatOpenAI(
            model=model_name,
            azure_endpoint=f"https://{azure_cfg.get('azureOpenAIApiInstanceName')}.openai.azure.com",
            api_key=azure_cfg.get("azureOpenAIApiKey"),
            api_version=azure_cfg.get("azureOpenAIApiVersion"),
            azure_deployment=azure_cfg.get("azureOpenAIApiDeploymentName"),
            temperature=kwargs.get("temperature"),
            max_tokens=kwargs.get("max_tokens"),
        )

    elif provider == "anthropic":
        from langchain_anthropic import ChatAnthropic

        # 重要: 覆盖 ChatAnthropic 默认的 top_p/top_k 避免 API 错误
        # ChatAnthropic 默认设置 top_p=1, top_k=1，这会与某些 API 配置冲突
        # 将 top_p/top_k 作为顶级参数设置为 None 来覆盖默认值
        anthropic_kwargs = {
            "model": model_name,
            "top_p": None,
            "top_k": None,
        }
        if not is_reasoning_model and final_temperature is not None:
            anthropic_kwargs["temperature"] = final_temperature
        if final_max_tokens is not None:
            anthropic_kwargs["max_tokens"] = final_max_tokens

        return ChatAnthropic(**anthropic_kwargs)

    elif provider == "google-genai":
        from langchain_google_genai import ChatGoogleGenerativeAI

        return ChatGoogleGenerativeAI(**kwargs)

    elif provider == "fireworks":
        from langchain_fireworks import ChatFireworks

        return ChatFireworks(**kwargs)

    elif provider == "groq":
        # Groq 使用 langchain-groq 包
        try:
            from langchain_groq import ChatGroq

            return ChatGroq(**kwargs)
        except ImportError:
            raise ImportError("langchain-groq package is required for Groq models")

    elif provider == "ollama":
        from langchain_ollama import ChatOllama

        base_url = model_cfg.get("baseUrl", "http://localhost:11434")
        return ChatOllama(model=model_name, base_url=base_url)

    else:
        raise ValueError(f"Unknown model provider: {provider}")


def optionally_get_system_prompt_from_config(config: RunnableConfig) -> Optional[str]:
    """从配置中获取自定义系统提示（如果有）"""
    return config.get("configurable", {}).get("systemPrompt")


def is_using_o1_mini_model(config: RunnableConfig) -> bool:
    """检查是否使用 o1-mini 模型"""
    model_config = get_model_config(config)
    return "o1-mini" in model_config.get("modelName", "")


# ============================================
# 消息格式化
# ============================================


def format_messages(messages: list[BaseMessage]) -> str:
    """
    格式化消息列表为 XML 字符串

    Args:
        messages: 消息列表

    Returns:
        XML 格式的消息字符串
    """
    formatted = []
    for idx, msg in enumerate(messages):
        msg_type = msg.type if hasattr(msg, "type") else "unknown"
        content = msg.content if isinstance(msg.content, str) else ""

        if not isinstance(msg.content, str) and hasattr(msg.content, "__iter__"):
            content = "\n".join(
                c.get("text", "") for c in msg.content if isinstance(c, dict) and "text" in c
            )

        formatted.append(f'<{msg_type} index="{idx}">\n{content}\n</{msg_type}>')

    return "\n".join(formatted)


def get_string_from_content(content: str | list[dict[str, Any]]) -> str:
    """从消息内容中提取字符串"""
    if isinstance(content, str):
        return content

    return "\n".join(
        c.get("text", "") for c in content if isinstance(c, dict) and "text" in c
    )


# ============================================
# 网络搜索结果处理
# ============================================


def create_ai_message_from_web_results(web_results: list[SearchResult]) -> AIMessage:
    """
    从网络搜索结果创建 AI 消息

    Args:
        web_results: 搜索结果列表 (嵌套结构: pageContent + metadata)

    Returns:
        包含搜索结果的 AIMessage

    Note:
        SearchResult 使用嵌套结构匹配 TS DocumentInterface<ExaMetadata>:
        - pageContent: 顶层字段
        - metadata: 包含 id, url, title, author, publishedDate 等
    """
    web_results_str = "\n\n".join(
        f"""<search-result
      index="{index}"
      publishedDate="{r.get('metadata', {}).get('publishedDate', 'Unknown')}"
      author="{r.get('metadata', {}).get('author', 'Unknown')}"
    >
      [{r.get('metadata', {}).get('title', 'Unknown title')}]({r.get('metadata', {}).get('url', 'Unknown URL')})
      {r.get('pageContent', '')}
    </search-result>"""
        for index, r in enumerate(web_results)
    )

    content = f"Here is some additional context I found from searching the web. This may be useful:\n\n{web_results_str}"

    return AIMessage(
        content=content,
        id=f"web-search-results-{uuid.uuid4()}",
        additional_kwargs={
            OC_WEB_SEARCH_RESULTS_MESSAGE_KEY: True,
            "webSearchResults": web_results,
            "webSearchStatus": "done",
        },
    )


# 注意: PROGRAMMING_LANGUAGES 已移至 constants.py
# 为保持向后兼容，通过 from .constants import PROGRAMMING_LANGUAGES 导入并重新导出


# ============================================
# 思考模型辅助函数
# ============================================


def is_thinking_model(model_name: str) -> bool:
    """
    检测是否为思考模型 (执行 CoT 推理的模型)

    Args:
        model_name: 模型名称

    Returns:
        是否为思考模型
    """
    from .constants import THINKING_MODELS

    return model_name in THINKING_MODELS


def extract_thinking_and_response(text: str) -> tuple[str, str]:
    """
    从思考模型输出中提取 thinking 和 response 部分

    支持流式输出 (标签可能不完整)

    Args:
        text: 模型输出文本

    Returns:
        (thinking, response) 元组

    Examples:
        >>> extract_thinking_and_response('Hello <think>processing...</think>world')
        ('processing...', 'Hello world')

        >>> extract_thinking_and_response('Hello <think>processing...')
        ('processing...', 'Hello ')

        >>> extract_thinking_and_response('Hello world')
        ('', 'Hello world')
    """
    think_start_tag = "<think>"
    think_end_tag = "</think>"

    start_index = text.find(think_start_tag)

    # 没有找到思考标签
    if start_index == -1:
        return ("", text.strip())

    after_start_tag = text[start_index + len(think_start_tag) :]
    end_index = after_start_tag.find(think_end_tag)

    # 没有结束标签 - 剩余文本都是思考内容
    if end_index == -1:
        return (after_start_tag.strip(), text[:start_index].strip())

    # 有开始和结束标签
    thinking = after_start_tag[:end_index].strip()
    response = (text[:start_index] + after_start_tag[end_index + len(think_end_tag) :]).strip()

    return (thinking, response)


# ============================================
# PDF 转换和上下文文档处理
# ============================================


def clean_base64(base64_string: str) -> str:
    """
    清理 base64 字符串，移除 data URL 前缀

    Args:
        base64_string: 可能包含 data URL 前缀的 base64 字符串

    Returns:
        纯净的 base64 字符串
    """
    if "base64," in base64_string:
        return base64_string.split("base64,", 1)[1]
    return base64_string


async def convert_pdf_to_text(base64_pdf: str) -> str:
    """
    将 base64 编码的 PDF 转换为文本

    Args:
        base64_pdf: Base64 编码的 PDF 数据

    Returns:
        提取的文本内容
    """
    import base64
    from io import BytesIO

    try:
        from pypdf import PdfReader

        cleaned_base64 = clean_base64(base64_pdf)
        pdf_bytes = base64.b64decode(cleaned_base64)
        pdf_reader = PdfReader(BytesIO(pdf_bytes))

        text_parts = []
        for page in pdf_reader.pages:
            text_parts.append(page.extract_text() or "")

        return "\n".join(text_parts)
    except ImportError:
        print("Warning: pypdf not installed, cannot convert PDF to text")
        return ""
    except Exception as e:
        print(f"Error converting PDF to text: {e}")
        raise


async def create_context_document_messages_openai(
    documents: list["ContextDocument"],
) -> list[dict]:
    """
    为 OpenAI 模型创建上下文文档消息

    OpenAI 不支持原生 PDF，所有文档转换为文本格式

    Args:
        documents: 上下文文档列表

    Returns:
        OpenAI 格式的消息内容列表
    """
    import base64

    messages = []
    for doc in documents:
        text = ""
        doc_type = doc.get("type", "")
        doc_data = doc.get("data", "")

        if doc_type == "application/pdf":
            text = await convert_pdf_to_text(doc_data)
        elif doc_type.startswith("text/"):
            cleaned = clean_base64(doc_data)
            text = base64.b64decode(cleaned).decode("utf-8")
        elif doc_type == "text":
            text = doc_data

        if text:
            messages.append({"type": "text", "text": text})

    return messages


async def create_context_document_messages_anthropic(
    documents: list["ContextDocument"],
    native_support: bool = False,
) -> list[dict]:
    """
    为 Anthropic 模型创建上下文文档消息

    Claude 3.5 Sonnet 支持原生 PDF，其他模型需要转换为文本

    Args:
        documents: 上下文文档列表
        native_support: 是否支持原生 PDF (3.5 sonnet)

    Returns:
        Anthropic 格式的消息内容列表
    """
    import base64

    messages = []
    for doc in documents:
        doc_type = doc.get("type", "")
        doc_data = doc.get("data", "")

        if doc_type == "application/pdf" and native_support:
            # 原生 PDF 支持 - 使用 document 格式
            messages.append({
                "type": "document",
                "source": {
                    "type": "base64",
                    "media_type": doc_type,
                    "data": clean_base64(doc_data),
                },
            })
        else:
            # 转换为文本
            text = ""
            if doc_type == "application/pdf" and not native_support:
                text = await convert_pdf_to_text(doc_data)
            elif doc_type.startswith("text/"):
                cleaned = clean_base64(doc_data)
                text = base64.b64decode(cleaned).decode("utf-8")
            elif doc_type == "text":
                text = doc_data

            if text:
                messages.append({"type": "text", "text": text})

    return messages


def create_context_document_messages_gemini(
    documents: list["ContextDocument"],
) -> list[dict]:
    """
    为 Google Gemini 模型创建上下文文档消息

    Gemini 支持原生 PDF 格式

    Args:
        documents: 上下文文档列表

    Returns:
        Gemini 格式的消息内容列表
    """
    import base64

    messages = []
    for doc in documents:
        doc_type = doc.get("type", "")
        doc_data = doc.get("data", "")

        if doc_type == "application/pdf":
            messages.append({
                "type": doc_type,
                "data": clean_base64(doc_data),
            })
        elif doc_type.startswith("text/"):
            cleaned = clean_base64(doc_data)
            text = base64.b64decode(cleaned).decode("utf-8")
            messages.append({"type": "text", "text": text})
        elif doc_type == "text":
            messages.append({"type": "text", "text": doc_data})
        else:
            raise ValueError(f"Unsupported document type: {doc_type}")

    return messages


async def get_context_documents(config: RunnableConfig) -> list["ContextDocument"]:
    """
    从 store 获取上下文文档

    Args:
        config: LangGraph 运行配置

    Returns:
        上下文文档列表
    """
    store = config.get("store")
    assistant_id = config.get("configurable", {}).get("assistant_id")

    if not store or not assistant_id:
        return []

    result = await store.aget(CONTEXT_DOCUMENTS_NAMESPACE, assistant_id)
    return result.value.get("documents", []) if result and result.value else []


async def create_context_document_messages(
    config: RunnableConfig,
    context_documents: list["ContextDocument"] | None = None,
) -> list[dict]:
    """
    为当前模型提供商创建上下文文档消息

    根据模型提供商 (OpenAI/Anthropic/Gemini) 选择正确的文档格式。

    Args:
        config: LangGraph 配置 (包含模型信息)
        context_documents: 可选的文档列表 (如果不提供，从 store 获取)

    Returns:
        包含 role='user' 和格式化文档内容的消息列表

    Note:
        返回格式:
        [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Use the file(s)..."},
                    ...document_messages...
                ]
            }
        ]
    """
    from .types import ContextDocument

    model_cfg = get_model_config(config)
    model_provider = model_cfg.get("modelProvider", "")
    model_name = model_cfg.get("modelName", "")

    documents: list[ContextDocument] = list(context_documents) if context_documents else []
    if not documents:
        documents = await get_context_documents(config)

    if not documents:
        return []

    # 根据提供商创建文档消息
    context_doc_messages: list[dict] = []

    if model_provider == "openai" or model_provider == "azure_openai":
        context_doc_messages = await create_context_document_messages_openai(documents)
    elif model_provider == "anthropic":
        # Claude 3.5 Sonnet 支持原生 PDF
        native_support = "3-5-sonnet" in model_name or "3.5-sonnet" in model_name
        context_doc_messages = await create_context_document_messages_anthropic(
            documents, native_support=native_support
        )
    elif model_provider == "google-genai":
        context_doc_messages = create_context_document_messages_gemini(documents)

    if not context_doc_messages:
        return []

    # 包装为用户消息
    return [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Use the file(s) and/or text below as context when generating your response.",
                },
                *context_doc_messages,
            ],
        }
    ]
